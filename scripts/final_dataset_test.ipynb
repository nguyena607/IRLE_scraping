{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498515f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37249ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/alyssanguyen/Desktop/IRLE_scraping/csv_files/raw_prices_carlsjr_ca_05152024.csv\"\n",
    "ca_ff = pd.read_csv(file_path)\n",
    "file_path_2 = \"/Users/alyssanguyen/Desktop/IRLE_scraping/csv_files/uszips.csv\"\n",
    "ca_zip_count = pd.read_csv(file_path_2)\n",
    "file_path_3 = \"/Users/alyssanguyen/Desktop/IRLE_scraping/csv_files/processed_prices_ubereats_ca_ff_03222024.csv\"\n",
    "example = pd.read_csv(file_path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a78e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all the columns we don't need \n",
    "ca_ff_ = ca_ff.drop(columns=['Unnamed: 0', 'inputted_location','restaurant_distance'])\n",
    "ca_ff_ = ca_ff_.dropna(subset=['restaurant_location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_zip_count = ca_zip_count[['zip', 'county_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e008b7",
   "metadata": {},
   "source": [
    "Exploring missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a56f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Nan values \n",
    "nan_count = ca_ff.isnull().sum()\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#restaurant_rating cleaning \n",
    "# Ensure the column is of string type using .loc\n",
    "ca_ff_.loc[:, 'restaurant_rating'] = ca_ff_['restaurant_rating'].astype(str)\n",
    "\n",
    "# Count rows containing 'mi'\n",
    "rows_with_mi = ca_ff_['restaurant_rating'].str.contains('mi').sum()\n",
    "print(\"Number of rows with 'mi' in restaurant rating:\", rows_with_mi)\n",
    "\n",
    "# Replace invalid ratings ending with 'mi' with '0' using .loc\n",
    "ca_ff_.loc[:, 'restaurant_rating'] = ca_ff_['restaurant_rating'].str.replace(r'.*mi$', '0', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting data types \n",
    "ca_ff_['restaurant_name'] = ca_ff_['restaurant_name'].astype('string')\n",
    "ca_ff_['menu_item'] = ca_ff_['menu_item'].astype('string')\n",
    "ca_ff_['menu_item'] = ca_ff_['menu_item'].str.replace(r'\\s+', ' ', regex=True)\n",
    "ca_ff_['restaurant_location'] = ca_ff_['restaurant_location'].astype('string')\n",
    "ca_ff_['restaurant_rating'] = ca_ff_['restaurant_rating'].str.strip().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f9396",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cleaning up string columns \n",
    "\n",
    "ca_ff_['menu_item'] = ca_ff_['menu_item'].str.lower()\n",
    "ca_ff_['restaurant_location'] = ca_ff_['restaurant_location'].str.lower()\n",
    "ca_ff_['restaurant_name'] = ca_ff_['restaurant_name'].str.replace('_', ' ')\n",
    "\n",
    "#remove special characters\n",
    "ca_ff_['menu_item'] = ca_ff_['menu_item'].apply(lambda x: ''.join(ch for ch in x if ch.isalnum() or ch.isspace()))\n",
    "ca_ff_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_list(x):\n",
    "    return list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_non_zero(x):\n",
    "    return np.mean(x[x != 0]) if np.any(x != 0) else 0\n",
    "\n",
    "def median_non_zero(x):\n",
    "    return np.median(x[x != 0]) if np.any(x != 0) else 0\n",
    "\n",
    "def std_non_zero(x):\n",
    "    return np.std(x[x != 0]) if np.any(x != 0) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd18a03",
   "metadata": {},
   "source": [
    "McDonald's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39db15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to just McDonalds\n",
    "ca_ff_mcd = ca_ff_[ca_ff_['restaurant_name'] == 'McDonald']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'mean', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_mcd = ca_ff_mcd.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_mcd.columns = [' '.join(col).strip() for col in grouped_mcd.columns.values]\n",
    "\n",
    "\n",
    "#Second part of grouping \n",
    "mcd_lst = ['big mac', 'big mac meal', 'cheeseburger', 'hamburger', 'medium french fries']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_mcd = ca_ff_mcd[ca_ff_mcd['menu_item'].isin(mcd_lst)].sort_values('menu_item')\n",
    "menu_items_mcd = menu_items_mcd.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_mcd_2 = menu_items_mcd.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_mcd_2[['specialty_item', 'combo', 'cheeseburger', 'hamburger', 'fries']] = grouped_mcd_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_mcd_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "#Merging the grouped dfs together \n",
    "merged_mcd = pd.merge(grouped_mcd, grouped_mcd_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "merged_mcd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13cb47",
   "metadata": {},
   "source": [
    "Jack in the Box "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to just Jack in the Box\n",
    "ca_ff_jack = ca_ff_[ca_ff_['restaurant_name'] == 'Jack in the Box']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'mean', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_jack = ca_ff_jack.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_jack.columns = [' '.join(col).strip() for col in grouped_jack.columns.values]\n",
    "\n",
    "# #Second part of grouping \n",
    "jack_lst = ['jr jumbo jack', 'jr jumbo jack cheeseburger', 'jumbo jack', 'large french fry', 'large jumbo jack combo']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_jack = ca_ff_jack[ca_ff_jack['menu_item'].isin(jack_lst)].sort_values('menu_item')\n",
    "menu_items_jack = menu_items_jack.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_jack_2 = menu_items_jack.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_jack_2[['hamburger', 'cheeseburger', 'specialty_item', 'fries', 'combo']] = grouped_jack_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_jack_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "# #Merging the grouped dfs together \n",
    "merged_jack = pd.merge(grouped_jack, grouped_jack_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "merged_jack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b93737",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_ff_jack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83e9b8",
   "metadata": {},
   "source": [
    "Wendy's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866926b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to just Wendy's\n",
    "ca_ff_wendy = ca_ff_[ca_ff_['restaurant_name'] == 'Wendy']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'first', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_wendy = ca_ff_wendy.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_wendy.columns = [' '.join(col).strip() for col in grouped_wendy.columns.values]\n",
    "\n",
    "# #Second part of grouping \n",
    "wendy_lst = ['daves combo', 'daves single', 'french fries', 'jr cheeseburger', 'jr hamburger']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_wendy = ca_ff_wendy[ca_ff_wendy['menu_item'].isin(wendy_lst)].sort_values('menu_item')\n",
    "menu_items_wendy = menu_items_wendy.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_wendy_2 = menu_items_wendy.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_wendy_2[['combo', 'specialty_item', 'fries', 'cheeseburger', 'hamburger']] = grouped_wendy_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_wendy_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "merged_wendy = pd.merge(grouped_wendy, grouped_wendy_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "merged_wendy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00796c3",
   "metadata": {},
   "source": [
    "Burger King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd2a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to just Burger King \n",
    "ca_ff_bk = ca_ff_[ca_ff_['restaurant_name'] == 'Burger King']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'mean', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_bk = ca_ff_bk.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_bk.columns = [' '.join(col).strip() for col in grouped_bk.columns.values]\n",
    "\n",
    "# #Second part of grouping \n",
    "#they don't have a plain hamburger FOR NOW using whopper jr \n",
    "bk_lst = ['cheeseburger', 'french fries', 'whopper', 'whopper jr', 'whopper meal']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_bk = ca_ff_bk[ca_ff_bk['menu_item'].isin(bk_lst)].sort_values('menu_item')\n",
    "menu_items_bk = menu_items_bk.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_bk_2 = menu_items_bk.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_bk_2[['cheeseburger', 'fries', 'specialty_item', 'hamburger', 'combo']] = grouped_bk_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_bk_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "merged_bk = pd.merge(grouped_bk, grouped_bk_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "merged_bk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b73053",
   "metadata": {},
   "source": [
    "Shake Shack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to just Shake Shack\n",
    "ca_ff_shake = ca_ff_[ca_ff_['restaurant_name'] == 'Shake Shack']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'mean', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_shake = ca_ff_shake.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_shake.columns = [' '.join(col).strip() for col in grouped_shake.columns.values]\n",
    "\n",
    "\n",
    "#Second part of grouping \n",
    "shake_lst = ['cheeseburger', 'fries', 'hamburger', 'shackburger']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_shake = ca_ff_shake[ca_ff_shake['menu_item'].isin(shake_lst)].sort_values('menu_item')\n",
    "menu_items_shake = menu_items_shake.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_shake_2 = menu_items_shake.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_shake_2[['cheeseburger', 'fries', 'hamburger', 'specialty_item']] = grouped_shake_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_shake_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "#Merging the grouped dfs together \n",
    "merged_shake = pd.merge(grouped_shake, grouped_shake_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "merged_shake['combo'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed4116",
   "metadata": {},
   "source": [
    "Sonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc158f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAG many missing prices for fries and \n",
    "#Filter to just Sonic \n",
    "ca_ff_sonic = ca_ff_[ca_ff_['restaurant_name'] == 'Sonic']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'mean', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_sonic = ca_ff_sonic.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_sonic.columns = [' '.join(col).strip() for col in grouped_sonic.columns.values]\n",
    "\n",
    "\n",
    "#Second part of grouping \n",
    "sonic_lst = ['fries', 'quarter pound double cheeseburger', 'supersonic double cheeseburger', 'supersonic double cheeseburger combo']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_sonic = ca_ff_sonic[ca_ff_sonic['menu_item'].isin(sonic_lst)].sort_values('menu_item')\n",
    "menu_items_sonic = menu_items_sonic.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_sonic_2 = menu_items_sonic.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_sonic_2[['fries', 'cheeseburger', 'specialty_item', 'combo']] = grouped_sonic_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_sonic_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "#Merging the grouped dfs together \n",
    "merged_sonic = pd.merge(grouped_sonic, grouped_sonic_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "merged_sonic['hamburger'] = np.nan\n",
    "merged_sonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fcd9c5",
   "metadata": {},
   "source": [
    "Five Guys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for Five Guys\n",
    "ca_ff_five = ca_ff_[ca_ff_['restaurant_name'] == 'Five Guys']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'mean', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_five = ca_ff_five.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_five.columns = [' '.join(col).strip() for col in grouped_five.columns.values]\n",
    "\n",
    "\n",
    "#Second part of grouping \n",
    "five_lst = ['cheeseburger', 'little cheeseburger', 'little hamburger', 'regular fries']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_five = ca_ff_five[ca_ff_five['menu_item'].isin(five_lst)].sort_values('menu_item')\n",
    "menu_items_five = menu_items_five.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_five_2 = menu_items_five.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_five_2[['specialty_item', 'cheeseburger', 'hamburger', 'fries']] = grouped_five_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_five_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "#Merging the grouped dfs together \n",
    "merged_five = pd.merge(grouped_five, grouped_five_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "merged_five['combo'] = np.nan\n",
    "merged_five"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad01463f",
   "metadata": {},
   "source": [
    "The Habit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26bb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_ff_habit = ca_ff_[ca_ff_['restaurant_name'] == 'The Habit']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'mean', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_habit = ca_ff_habit.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_habit.columns = [' '.join(col).strip() for col in grouped_habit.columns.values]\n",
    "\n",
    "\n",
    "#Second part of grouping \n",
    "habit_lst = ['2 original double char meal', 'charburger', 'charburger with cheese', 'double char', 'french fries']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_habit = ca_ff_habit[ca_ff_habit['menu_item'].isin(habit_lst)].sort_values('menu_item')\n",
    "menu_items_habit = menu_items_habit.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_habit_2 = menu_items_habit.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_habit_2[['combo', 'hamburger', 'cheeseburger', 'specialty_item','fries']] = grouped_habit_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_habit_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "#Merging the grouped dfs together \n",
    "merged_habit = pd.merge(grouped_habit, grouped_habit_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "#merged_five['combo'] = np.nan\n",
    "merged_habit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51a68e",
   "metadata": {},
   "source": [
    "Carl's Jr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f9db6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Filter to Carl's Jr \n",
    "\n",
    "ca_ff_carls = ca_ff_[ca_ff_['restaurant_name'] == 'Carls Jr']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'mean', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_carls = ca_ff_carls.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_carls.columns = [' '.join(col).strip() for col in grouped_carls.columns.values]\n",
    "\n",
    "\n",
    "#Second part of grouping \n",
    "carls_lst = ['california classic double cheeseburger', 'naturalcut french fries', 'single big carl', 'single big carl combo']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_carls = ca_ff_carls[ca_ff_carls['menu_item'].isin(carls_lst)].sort_values('menu_item')\n",
    "menu_items_carls = menu_items_carls.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_carls_2 = menu_items_carls.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_carls_2[['cheeseburger', 'fries', 'specialty_item','combo']] = grouped_carls_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_carls_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "#Merging the grouped dfs together \n",
    "merged_carls = pd.merge(grouped_carls, grouped_carls_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "merged_carls['hamburger'] = np.nan\n",
    "merged_carls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to Hardees's Jr \n",
    "\n",
    "ca_ff_hardee = ca_ff_[ca_ff_['restaurant_name'] == 'Hardee']\n",
    "\n",
    "#First part of grouping \n",
    "\n",
    "agg_funcs = {\n",
    "    'menu_item_price': [mean_non_zero, median_non_zero, std_non_zero],  # calculate the average, median, and standard dev PRICE\n",
    "    'restaurant_rating': 'mean', # calculate the average RATING \n",
    "    'menu_item' : 'count',\n",
    "    'number_of_ratings': 'first'\n",
    "}\n",
    "\n",
    "grouped_hardee = ca_ff_hardee.groupby(['restaurant_name','restaurant_location']).agg(agg_funcs).reset_index()\n",
    "grouped_hardee.columns = [' '.join(col).strip() for col in grouped_hardee.columns.values]\n",
    "\n",
    "\n",
    "#Second part of grouping \n",
    "carls_lst = ['california classic double cheeseburger', 'naturalcut french fries', 'single big carl', 'single big carl combo']\n",
    "\n",
    "# Filter rows where 'menu_item' contains any item in mcd_lst\n",
    "menu_items_hardee = ca_ff_hardee[ca_ff_hardee['menu_item'].isin(carls_lst)].sort_values('menu_item')\n",
    "menu_items_hardee = menu_items_hardee.drop_duplicates(subset=['restaurant_name', 'restaurant_location', 'menu_item'])\n",
    "\n",
    "grouped_hardee_2 = menu_items_hardee.groupby(['restaurant_name', 'restaurant_location'])['menu_item_price'].agg(price_list).reset_index()\n",
    "\n",
    "grouped_hardee_2[['cheeseburger', 'fries', 'specialty_item','combo']] = grouped_hardee_2['menu_item_price'].apply(pd.Series)\n",
    "grouped_hardee_2.drop(columns=['menu_item_price'], inplace=True)\n",
    "\n",
    "#Merging the grouped dfs together \n",
    "merged_hardee = pd.merge(grouped_hardee, grouped_hardee_2, on=['restaurant_name', 'restaurant_location'], how='inner')\n",
    "#merged_carls['hamburger'] = np.nan\n",
    "merged_carls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in unique_menu_items:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ef8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack all restaurants\n",
    "uber_eats_ff_rnd1_prices = pd.concat([merged_mcd, merged_jack, merged_wendy, merged_shake, merged_bk, merged_sonic, merged_carls, merged_habit, merged_five]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If there are bad addresses, replace them with the actual address here \n",
    "\n",
    "uber_eats_ff_rnd1_prices.loc[1012, 'restaurant_location'] = \"s64w15924 commerce center parkway, muskego, wi, 53150\"\n",
    "uber_eats_ff_rnd1_prices.loc[1755, 'restaurant_location'] = \"860 peachtree rd ne, atlanta, ga, 30308\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9739a",
   "metadata": {},
   "source": [
    "Add location columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\",\\s*([a-zA-Z]{2})\\s*,?\\s*(\\d{5}(?:-\\d{4})?)\"\n",
    "\n",
    "def extract_state_zip(address):\n",
    "    match = re.search(pattern, address)\n",
    "    if match:\n",
    "        state, zip_code = match.groups()\n",
    "        return state, zip_code\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to extract state and zip code\n",
    "uber_eats_ff_rnd1_prices[['state', 'zip']] = uber_eats_ff_rnd1_prices['restaurant_location'].apply(lambda x: pd.Series(extract_state_zip(x)))\n",
    "uber_eats_ff_rnd1_prices['zip'] = uber_eats_ff_rnd1_prices['zip'].str.split('-').str[0].astype(int)\n",
    "\n",
    "#Get county \n",
    "uber_eats_ff_rnd1_prices = uber_eats_ff_rnd1_prices.merge(ca_zip_count, on = 'zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124da92",
   "metadata": {},
   "source": [
    "Dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_date = datetime.strptime('05152024', '%m%d%Y')\n",
    "# Assign the datetime object to the entire 'date' column\n",
    "uber_eats_ff_rnd1_prices['date'] = specific_date\n",
    "uber_eats_ff_rnd1_prices['uber_eats'] = 1 \n",
    "uber_eats_ff_rnd1_prices['post_policy'] = 1\n",
    "uber_eats_ff_rnd1_prices['fast_food'] = 1 \n",
    "uber_eats_ff_rnd1_prices['local'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save as csv \n",
    "uber_eats_ff_rnd1_prices.to_csv('processed_prices_carlsjr_ca_05152024.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
